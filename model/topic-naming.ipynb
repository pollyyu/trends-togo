{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geoplotlib\n",
    "# !pip install folium --user\n",
    "# !pip install geopandas --user\n",
    "# !pip install pandabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_complete.pickle','rb') as read_file:\n",
    "    df_complete = pickle.load(read_file)\n",
    "    \n",
    "with open('cuisine_topic_info.pickle','rb') as read_file:\n",
    "    cuisine_topic_info = pickle.load(read_file)\n",
    "    \n",
    "with open('dish_topic_info.pickle','rb') as read_file:\n",
    "    dish_topic_info = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curating information for Topic Naming\n",
    "\n",
    "Now that we have clustered the data into 30 restaurant profiles, I am going to name them using the same information we fed into our K-means model\n",
    "\n",
    "**Cuisines:** Remember the LDA topic modeling earlier? The cuisine topic-keyword distribution and the document-topic distribution became useful in analyzing each clusters. I am going to get each cluster's top 3 Cuisine dimensions by their key topic keywords and percentage contribution to the final k-means clustering.\\\n",
    "**Dishes:** The same process as cuisines but with dishes LDA topic modeling\\\n",
    "**Price:** Get the average max price for each cluster\\\n",
    "**Menu Variability:** Get the average count of top dishes for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting cuisine representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_label_grped = df_complete.groupby(['km_label','cuisine_dominant_topic']).count()\n",
    "tot = df_complete.groupby(['km_label']).count().reset_index()\n",
    "\n",
    "# km_label_grped\n",
    "g = km_label_grped['row_number'].groupby(level=0, group_keys=False)\n",
    "res = g.apply(lambda x: x.sort_values(ascending=False).head(3)).reset_index()\n",
    "merged = pd.merge(res, tot, how='left',on='km_label')\n",
    "merged['perc'] = merged['row_number_x']/merged['row_number_y']\n",
    "\n",
    "cuisine_inf = pd.merge(merged[['km_label','cuisine_dominant_topic_x','row_number_x','perc']],cuisine_topic_info, how='left',\n",
    "        left_on='cuisine_dominant_topic_x', right_on = 'cuisine_dominant_topic')\n",
    "\n",
    "cuisine_inf = cuisine_inf[['km_label','cuisine_dominant_topic_x','row_number_x','perc','top_doc_cuisines','cuisine_topic_keywords']]\n",
    "cuisine_inf.columns = ['km_label','cuisine_dominant_topic','cuisine_topic_count','cuisine_topic_perc','top_doc_cuisines',\n",
    "                       'cuisine_topic_keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting dish representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_label_grped = df_complete.groupby(['km_label','dish_dominant_topic']).count()\n",
    "tot = df_complete.groupby(['km_label']).count().reset_index()\n",
    "\n",
    "# km_label_grped\n",
    "g = km_label_grped['row_number'].groupby(level=0, group_keys=False)\n",
    "res = g.apply(lambda x: x.sort_values(ascending=False).head(3)).reset_index()\n",
    "merged = pd.merge(res, tot, how='left',on='km_label')\n",
    "merged['perc'] = merged['row_number_x']/merged['row_number_y']\n",
    "\n",
    "dish_inf = pd.merge(merged[['km_label','dish_dominant_topic_x','row_number_x','perc']],dish_topic_info, how='left',\n",
    "        left_on='dish_dominant_topic_x', right_on = 'dish_dominant_topic')\n",
    "dish_inf = dish_inf[['km_label','dish_dominant_topic_x','row_number_x','perc','top_doc_dish','dish_topic_keywords']]\n",
    "dish_inf.columns = ['km_label1','dish_dominant_topic','dish_topic_count','dish_topic_perc','top_doc_dish',\n",
    "                       'dish_topic_keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting price inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_inf = df_complete.groupby('km_label').agg({'row_number':'count','price_rating_x':'median','max_price':'median',\n",
    "                                     'count_top_dishes':'median','unique_dishes':'median',\n",
    "                                     'unique_hashtag_count':'median'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining Cuisine and Dish Interpretation into one table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = pd.merge(pd.concat([cuisine_inf, dish_inf], axis=1), restaurant_inf, how = 'left', on = 'km_label')\n",
    "\n",
    "## saving inference into excel\n",
    "inference_df.to_excel(\"inference2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import *\n",
    "import pandabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:vF_7gjg>n9.a\"F(PS\\'#YjaWDz,mz]=4XJUT5Tr(Z2FVx\\'+WMQs@54.153.84.135:5432/grubhub')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving df_complete to sql table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = df_complete[['restaurant_id','name', 'restaurant_hash','city','long','lat','max_price',\n",
    "                         'count_top_dishes', 'unique_dishes', 'unique_hashtag_count', 'km_label',\n",
    "                           'dish_dominant_topic', 'dish_perc_contribution', 'dish_topic_keywords',\n",
    "                           'dishes', 'cuisine_dominant_topic', 'cuisine_perc_contribution',\n",
    "                           'cuisine_topic_keywords', 'cuisine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_labeled['dishes'] = df_labeled['dishes'].apply(lambda x: str(x).replace(\"[\",\"\").replace(\"]\",\"\"))\n",
    "df_labeled['cuisine'] = df_labeled['cuisine'].apply(lambda x: str(x).replace(\"[\",\"\").replace(\"]\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot upsert with an automatic index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-242d57932b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnamed_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inference2.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpandabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamed_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'named_topics'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upsert'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mauto_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandabase/pandabase.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(df, table_name, con, auto_index, how, add_new_columns, schema)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'upsert'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_key\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPANDABASE_DEFAULT_INDEX\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mauto_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot upsert with an automatic index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# 3. iterate over df_columns; confirm that types are compatible and all columns exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot upsert with an automatic index"
     ]
    }
   ],
   "source": [
    "named_topics = pd.read_csv('inference2.csv', index_col=0)\n",
    "\n",
    "pandabase.to_sql(named_topics, table_name='named_topics', con=engine,how='upsert',auto_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot upsert with an automatic index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fd35e944e54d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpandabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'df_labeled2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upsert'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mauto_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandabase/pandabase.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(df, table_name, con, auto_index, how, add_new_columns, schema)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'upsert'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_key\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPANDABASE_DEFAULT_INDEX\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mauto_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot upsert with an automatic index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# 3. iterate over df_columns; confirm that types are compatible and all columns exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot upsert with an automatic index"
     ]
    }
   ],
   "source": [
    "pandabase.to_sql(df_labeled, table_name='df_labeled2', con=engine,how='upsert',auto_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting cuisine representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After naming each of the 30 topics, upload it to our SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the city labels\n",
    "df_complete['city'] = df_complete['city'].apply(lambda x: x.strip())\n",
    "\n",
    "# group df with city and their km_labels\n",
    "df_cities = df_complete.groupby(['city','km_label']).agg({'row_number':'count','rating_count':['sum','mean','median']}).reset_index()\n",
    "df_cities.columns = ['city','km_label','count','sum_reviews','mean_reviews','median_reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the biggest cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>New York</td>\n",
       "      <td>3273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Chicago</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>1904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>1488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>1041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>Houston</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Miami</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Bronx</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>Portland</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>Washington</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Denver</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Dallas</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Boston</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>Jersey City</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>Staten Island</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Glendale</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Astoria</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              city  count\n",
       "553       New York   3273\n",
       "139        Chicago   1974\n",
       "91        Brooklyn   1904\n",
       "465    Los Angeles   1488\n",
       "634   Philadelphia   1041\n",
       "358        Houston    697\n",
       "508          Miami    643\n",
       "87           Bronx    517\n",
       "734        Seattle    513\n",
       "649       Portland    464\n",
       "839     Washington    433\n",
       "192         Denver    400\n",
       "26         Atlanta    373\n",
       "182         Dallas    345\n",
       "74          Boston    317\n",
       "637        Phoenix    294\n",
       "385    Jersey City    225\n",
       "766  Staten Island    221\n",
       "284       Glendale    213\n",
       "24         Astoria    213"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.groupby('city')['count'].sum().reset_index().sort_values(by='count',ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
